{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "txt = 'train.txt'\n",
    "with open(txt, 'r') as file:\n",
    "    data = file.readlines()\n",
    "validation_len = len(data)/5\n",
    "validation_set = data[0:validation_len]\n",
    "train_set = data[validation_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.714146341463 recall: 0.644366197183 F: 0.677464136974\n",
      "precision: 0.788229111929 recall: 0.653025685677 F: 0.714285714286\n",
      "precision: 0.842325581395 recall: 0.709639498433 F: 0.770310506168\n",
      "precision: 0.720283257461 recall: 0.656221198157 F: 0.686761514348\n",
      "precision: 0.86102062975 recall: 0.651334702259 F: 0.741641337386\n",
      "precision: 0.738916256158 recall: 0.617848970252 F: 0.67298105683\n",
      "precision: 0.790966386555 recall: 0.652230402772 F: 0.714929978638\n",
      "precision: 0.761761761762 recall: 0.667836770513 F: 0.711713818097\n",
      "precision: 0.803813346713 recall: 0.682573498083 F: 0.738248847926\n",
      "precision: 0.806574559314 recall: 0.684593610999 F: 0.740594925634\n",
      "avg_precision: 0.78280372325 recall: 0.661967053433 F: 0.716893183629\n"
     ]
    }
   ],
   "source": [
    "# Training and validation\n",
    "precision_avg = 0\n",
    "recall_avg = 0\n",
    "F_measure_avg = 0\n",
    "for i in range(1,11):\n",
    "    # cross validation\n",
    "    validation_len = len(data)/10\n",
    "    validation_set = data[(i-1)*validation_len:i*validation_len]\n",
    "    train_set = data[0:(i-1)*validation_len]\n",
    "    train_set.extend(data[i*validation_len:len(data)])\n",
    "    \n",
    "    \n",
    "    # build a lexicon that contains all the entities in the training set\n",
    "    word_NE = {}\n",
    "    for i in range(0,len(train_set),3):\n",
    "        word_line = train_set[i].replace('\\r\\n','').split('\\t')\n",
    "        NE_line = train_set[i+2].replace('\\r\\n','').split('\\t')\n",
    "\n",
    "\n",
    "        NE = []\n",
    "        begin = False\n",
    "        for j in range(0,len(word_line)):\n",
    "            word = word_line[j]\n",
    "            ne = NE_line[j]\n",
    "\n",
    "            if j==0 and ne[0]=='B':\n",
    "                NE.append(word)\n",
    "                begin = True\n",
    "            if j>0 and ne[0]=='I' and begin == True:\n",
    "                NE.append(word)\n",
    "                if j==len(word_line)-1:\n",
    "                    word_NE[tuple(NE)]=ne[2:]\n",
    "                    NE = []\n",
    "            if j>0 and ne[0]!='I' and begin == True:\n",
    "                word_NE[tuple(NE)]=NE_line[j-1][2:]\n",
    "                NE = []\n",
    "                begin = False\n",
    "            if j>0 and ne[0]=='B':\n",
    "                NE.append(word)\n",
    "                begin = True\n",
    "                if j==len(word_line)-1:\n",
    "                    word_NE[tuple(NE)]=NE_line[j][2:]\n",
    "                    NE = []\n",
    "                    \n",
    "    word_NE_prefix = {}\n",
    "    for t in word_NE.keys():\n",
    "        if t[0] not in word_NE_prefix:\n",
    "            word_NE_prefix[t[0]]=[t[0:]]\n",
    "        else:\n",
    "            word_NE_prefix[t[0]].append(t[0:])\n",
    "    \n",
    "    # identify only those named entities that are part of the lexicon above \n",
    "    correct = 0\n",
    "    all_correct = 0\n",
    "    predicted = 0\n",
    "    for i in range(0,len(validation_set),3):\n",
    "        word_line = validation_set[i].replace('\\r\\n','').split('\\t')\n",
    "        NE_line = validation_set[i+2].replace('\\r\\n','').split('\\t')\n",
    "\n",
    "        for j in range(0,len(word_line)):\n",
    "            word = word_line[j]\n",
    "            ne = NE_line[j]\n",
    "            \n",
    "            if ne[0] == 'B':\n",
    "                all_correct +=1\n",
    "        \n",
    "        end = 0\n",
    "        for j in range(0,len(word_line)):\n",
    "            word = word_line[j]\n",
    "            ne = NE_line[j]    \n",
    "            \n",
    "            if word in word_NE_prefix and j>=end:\n",
    "                match_list = word_NE_prefix[word]\n",
    "                for t in match_list:\n",
    "                    end_index = min(j+len(t),len(word_line))\n",
    "                    if tuple(word_line[j:end_index]) == t:\n",
    "                        predicted +=1\n",
    "                        tag = word_NE[t]\n",
    "                        end = end_index\n",
    "                        if tag == ne[2:]:\n",
    "                            correct +=1\n",
    "                        break\n",
    "        \n",
    "                        \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "    precision = correct/float(predicted)\n",
    "    recall  = correct/float(all_correct)\n",
    "    F_measure = 2*precision*recall/(precision + recall)\n",
    "    print \"precision: \"+str(precision), \"recall: \"+str(recall),\"F: \"+str(F_measure)\n",
    "    \n",
    "    precision_avg += precision\n",
    "    recall_avg += recall\n",
    "    F_measure_avg += F_measure\n",
    "\n",
    "print \"avg_precision: \"+str(precision_avg/10.0),\"recall: \"+str(recall_avg/10.0),\"F: \"+str(F_measure_avg/10.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper function to output the NE results\n",
    "def tagging(begin_index,end_index,tag):\n",
    "    global PER,LOC,ORG,MISC\n",
    "    if tag == 'PER':\n",
    "         PER =  PER+str(begin_index)+'-'+str(end_index)+' '\n",
    "    if tag == 'LOC':\n",
    "        LOC =  LOC+str(begin_index)+'-'+str(end_index)+' '\n",
    "    if tag == 'ORG':\n",
    "        ORG =  ORG+str(begin_index)+'-'+str(end_index)+' '\n",
    "    if tag == 'MISC':\n",
    "        MISC = MISC+str(begin_index)+'-'+str(end_index)+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# predict the test data and output the result\n",
    "\n",
    "headers = ['Type','Prediction']\n",
    "rows = []\n",
    "PER = ''\n",
    "LOC = ''\n",
    "ORG = ''\n",
    "MISC = ''\n",
    "\n",
    "txt = 'train.txt'\n",
    "with open(txt, 'r') as file:\n",
    "    train_set = file.readlines()\n",
    "\n",
    "word_NE = {}\n",
    "for i in range(0,len(train_set),3):\n",
    "    word_line = train_set[i].replace('\\r\\n','').split('\\t')\n",
    "    NE_line = train_set[i+2].replace('\\r\\n','').split('\\t')\n",
    "\n",
    "\n",
    "    NE = []\n",
    "    begin = False\n",
    "    for j in range(0,len(word_line)):\n",
    "        word = word_line[j]\n",
    "        ne = NE_line[j]\n",
    "\n",
    "        if j==0 and ne[0]=='B':\n",
    "            NE.append(word)\n",
    "            begin = True\n",
    "        if j>0 and ne[0]=='I' and begin == True:\n",
    "            NE.append(word)\n",
    "            if j==len(word_line)-1:\n",
    "                word_NE[tuple(NE)]=ne[2:]\n",
    "                NE = []\n",
    "        if j>0 and ne[0]!='I' and begin == True:\n",
    "            word_NE[tuple(NE)]=NE_line[j-1][2:]\n",
    "            NE = []\n",
    "            begin = False\n",
    "        if j>0 and ne[0]=='B':\n",
    "            NE.append(word)\n",
    "            begin = True\n",
    "            if j==len(word_line)-1:\n",
    "                word_NE[tuple(NE)]=NE_line[j][2:]\n",
    "                NE = []\n",
    "\n",
    "word_NE_prefix = {}\n",
    "for t in word_NE.keys():\n",
    "    if t[0] not in word_NE_prefix:\n",
    "        word_NE_prefix[t[0]]=[t[0:]]\n",
    "    else:\n",
    "        word_NE_prefix[t[0]].append(t[0:])\n",
    "            \n",
    "txt = 'test.txt'\n",
    "with open(txt, 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "for i in range(0,len(data),3):\n",
    "        word_line = data[i].replace('\\r\\n','').split('\\t')\n",
    "        num_line = data[i+2].replace('\\r\\n','').split()\n",
    "\n",
    "        \n",
    "        end = 0\n",
    "        for j in range(0,len(word_line)):\n",
    "            word = word_line[j]\n",
    "            \n",
    "            if word in word_NE_prefix and j>=end:\n",
    "                match_list = word_NE_prefix[word]\n",
    "                for t in match_list:\n",
    "                    end_index = min(j+len(t),len(word_line))\n",
    "                    if tuple(word_line[j:end_index]) == t:\n",
    "                        end = end_index\n",
    "                        tag = word_NE[t]  \n",
    "                        tagging(num_line[j],num_line[end_index-1],tag)\n",
    "                        break\n",
    "\n",
    "rows.append(('PER',PER))\n",
    "rows.append(('LOC',LOC))\n",
    "rows.append(('ORG',ORG))\n",
    "rows.append(('MISC',MISC))\n",
    "               \n",
    "            \n",
    "with open('baseline.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(rows)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
